# -*- coding: utf-8 -*-
"""ML Final Dor and Anthony.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wJdYIYf0iqxa0ege1Sa0cHcOridWqbOF

<h1 style="font-size:3rem;color:orange;">ML Final Assignment</h1>

<h1 style="font-size:1rem;color:white;">Writen by: Dor Gaon  & Anthony Tyzhay </h1>

Importing libraries:
"""

import pandas as pd
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Lambda
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.callbacks import ReduceLROnPlateau
from nltk.corpus import stopwords
import nltk
from collections import Counter
import re
from tensorflow.keras.models import load_model
from nltk.tokenize import word_tokenize
import numpy as np
import random
from tensorflow.keras.preprocessing.sequence import skipgrams
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import tensorflow as tf
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from keras_tuner import HyperModel
from keras_tuner.tuners import RandomSearch
import keras
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans

"""Initiating Pandas DF:"""

# Enable unsafe deserialization
keras.config.enable_unsafe_deserialization()
path = r'C:\Users\97252\Desktop\uni\msc\2024B\ML\FP\review_230k.parquet'
df = pd.read_parquet(path)
print(df.head(3))
print(df.info())

"""Setting random seed:"""

np.random.seed(42)
tf.random.set_seed(42)

"""Setting up stopwords:"""

# Import nltk stopwords
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))
custom_stopwords = ['would', 'could', 'might']
stop_words.update(custom_stopwords)

"""defining a function that removes punctuation, numbers ana spaces (data cleaning):"""

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation and numbers
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

"""Defining a tokenizing function:"""

def tokenize_review(review):
    return nltk.word_tokenize(review)

"""Applying data cleaning and tokeniztion functions to our data. we will use only the text column for training the model, as trying to combine the title and the text results in lose of context:"""

# Apply preprocessing and tokenization to title and text columns
#df['clean_title'] = df['title'].apply(lambda x: ' '.join(tokenize_review(preprocess_text(x))))
df['clean_text'] = df['text'].apply(lambda x: ' '.join(tokenize_review(preprocess_text(x))))

# Combine title and text for each review
df['combined_text'] = df['clean_text']

# Create a list of all words in the corpus
all_words = ' '.join(df['combined_text']).split()

# Display some statistics
print(f"Total number of reviews: {len(df)}")
print(f"Total number of words: {len(all_words)}")
print(f"Number of unique words: {len(set(all_words))}")

"""Defining "create vocabulary" function to create word to index and index to words mapping, while removing stopwords and rare words (minimum count is later set to 10). All removed words are replaced with special tokens (<Stop> for stop words, <UNK> for rare words). Also, we use padding tokens to make the lenght of the inputs fixed."""

# Create vocabulary function
def create_vocabulary(all_words, min_count, stop_words):
    word_counts = Counter(all_words)
    word_to_index = {}
    index_to_word = {}
    index = 0

    # Filter out rare words and stopwords
    for word, count in word_counts.items():
        if count >= min_count and word not in stop_words:
            word_to_index[word] = index
            index_to_word[index] = word
            index += 1

    # Add special tokens
    special_tokens = ['<PAD>', '<UNK>', '<STOP>']
    for special_token in special_tokens:
        word_to_index[special_token] = index
        index_to_word[index] = special_token
        index += 1

    return word_to_index, index_to_word

"""Defining a function to encode the dataset:"""

def encode_dataset(dataset, word_to_index):
    encoded_dataset = []
    for review in dataset:
        encoded_review = []
        tokens = tokenize_review(review)
        for word in tokens:
            encoded_review.append(word_to_index.get(word, word_to_index['<UNK>']))
        encoded_dataset.append(encoded_review)
    return encoded_dataset

"""Creating our vocabulary:"""

# Create vocabulary
min_count = 25
word_to_index, index_to_word = create_vocabulary(all_words, min_count, stop_words)

print(f"Vocabulary size: {len(word_to_index)}")

"""Emcode the dataset and convert to numpy:"""

encoded_dataset = encode_dataset(df['combined_text'], word_to_index)

# Convert to numpy array
encoded_dataset = np.array(encoded_dataset, dtype=object)

print(f"Shape of encoded dataset: {encoded_dataset.shape}")
print(f"Sample encoded review: {encoded_dataset[0][:20]}")

"""Print example mappings and word statistics:"""

print("\nSome word-to-index mappings:")
for word in list(word_to_index.keys())[:5]:
    print(f"{word}: {word_to_index[word]}")

print("\nSome index-to-word mappings:")
for index in list(index_to_word.keys())[:5]:
    print(f"{index}: {index_to_word[index]}")

word_counts = Counter(all_words)
total_words = sum(word_counts.values())
stop_words_count = sum(word_counts[word] for word in stop_words if word in word_counts)
print(f"\nTotal words: {total_words}")
print(f"Stop words count: {stop_words_count}")
print(f"Percentage of stop words: {stop_words_count/total_words*100:.2f}%")

"""Defining subampling function - since even after removing the stopwords we still encounter some non-informative words (and can't manualy remove all of them) we will try to make them less dominant in the training using subsampling:"""

# Set the subsampling threshold
subsampling_threshold = 1e-3

# Subsampling probability of each word
def subsample_word_prob(word, word_counts, total_words, threshold=subsampling_threshold):
    freq = word_counts[word] / total_words
    if freq > threshold:
        return 1 - np.sqrt(threshold / freq)
    else:
        return 0

"""Creating a dictionary of the words and the probabilities of them being subsampled:"""

# Create a dictionary with subsampling probabilities for each word
subsampling_probs = {
    word: subsample_word_prob(word, word_counts, total_words)
    for word in word_counts
}

"""Defining the data generator for the Word2Vec skipgram model:"""

special_tokens = {'<STOP>', '<UNK>', '<PAD>'}

def generate_training_data_generator(encoded_dataset, number_of_neg_samples: int, window_size: int):
    vocabulary_size = len(word_to_index)

    for encoded_review in encoded_dataset:
        review_length = len(encoded_review)
        if review_length < window_size * 2 + 1:
            continue

        # Subsample words
        subsampled_review = []
        for word_idx in encoded_review:
            word = index_to_word[word_idx]
            subsample_prob = subsampling_probs.get(word, 0)  # Get subsampling probability

            # Apply subsampling and skip special tokens
            if word not in special_tokens and random.random() > subsample_prob:
                subsampled_review.append(word_idx)  # Keep the word if it's not subsampled

        # If the review becomes too short after subsampling, skip it
        if len(subsampled_review) < window_size * 2 + 1:
            continue

        # Generate positive pairs from the subsampled review
        for i, target_word in enumerate(subsampled_review):
            # Skip if the target word is a special token
            if index_to_word[target_word] in special_tokens:
                continue

            start = max(0, i - window_size)
            end = min(len(subsampled_review), i + window_size + 1)

            for j in range(start, end):
                context_word = subsampled_review[j]
                # Skip if the context word is a special token or same as target
                if i != j and index_to_word[context_word] not in special_tokens:
                    yield (target_word, context_word), 1  # Positive pair

            # Negative samples generation
            negative_samples = set()
            while len(negative_samples) < number_of_neg_samples:
                neg_word = random.randint(0, vocabulary_size - 1)
                # Exclude special tokens and words within the context window from negative sampling
                if neg_word not in subsampled_review[start:end] and index_to_word[neg_word] not in special_tokens:
                    negative_samples.add(neg_word)

            # Yield negative pairs
            for neg_word in negative_samples:
                yield (target_word, neg_word), 0  # Negative pair

"""Defining the Word2Vec Skipgram model:"""

def build_skipgram_model(vocabulary_size, embedding_dim, learning_rate):
    target_word_input = Input(shape=(1,), dtype='int32') #input layer for the target word (we will pedict the words around it)
    context_word_input = Input(shape=(1,), dtype='int32')#input layer for the context word

    # Embedding layer with regularization
    embedding_layer = Embedding(input_dim=vocabulary_size,
                                output_dim=embedding_dim,
                                embeddings_regularizer=l2(1e-5),  # we used l2 regularization to prevent overfitting
                                embeddings_initializer='glorot_uniform', #initialization method
                                name="word_embedding")

    # Get embeddings for target and context
    target_embedding = embedding_layer(target_word_input)
    context_embedding = embedding_layer(context_word_input)

    # Flatten embeddings for computing the dot product in the next step
    target_embedding = Flatten()(target_embedding)
    context_embedding = Flatten()(context_embedding)

    # Apply L2 normalization to embeddings - we scale the embedding vectors to a unit form for improved learning process
    target_embedding = Lambda(lambda x: tf.nn.l2_normalize(x, axis=1), output_shape=lambda s: s)(target_embedding)
    context_embedding = Lambda(lambda x: tf.nn.l2_normalize(x, axis=1), output_shape=lambda s: s)(context_embedding)

    # Compute dot product between target and context embeddings - this indicates the simmilarity between the target and context words
    dot_product = Dot(axes=1)([target_embedding, context_embedding])

    # normalization of dot product
    dot_product = BatchNormalization()(dot_product)

    # Output layer with sigmoid activation function - ew make a binary classification - 1 - real pair 0 - not a real pair (neg sample)
    output = Dense(1, activation='sigmoid')(dot_product)

    # Compile the model
    model = Model(inputs=[target_word_input, context_word_input], outputs=output)
    #We use ADAM optimizer for adapting learning rate
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])

    return model

"""Setting the hyperparameters. those Hyperparameters were found as best using randomsearch with 10 different combinations. We will attach this at the end of the notebook."""

# Adjust training parameters
batch_size = 320  # best 320
steps_per_epoch = len(encoded_dataset) // batch_size
embedding_dim = 100  # best 100
window_size = 2  # best 2
number_of_neg_samples = 15  # best 15
learning_rate = 0.0001
epochs = 30

"""Defining the data generator (uses generate_training_data_generator to generate batches) :"""

# Generator for training data
def data_generator(batch_size):
    batch_input_target = []
    batch_input_context = []
    batch_labels = []
    while True:
        # Generate positive and negative pairs from the dataset
        for (target_word, context_word), label in generate_training_data_generator(encoded_dataset, number_of_neg_samples, window_size):
            batch_input_target.append(target_word)
            batch_input_context.append(context_word)
            batch_labels.append(label)

            # When we have a full batch, yield it
            if len(batch_input_target) == batch_size:
                # Ensure inputs (target and context words) are 1D arrays (batch_size,)
                batch_input_target = np.array(batch_input_target, dtype=np.int32).reshape(-1)
                batch_input_context = np.array(batch_input_context, dtype=np.int32).reshape(-1)

                # Labels are also 1D arrays (batch_size,)
                batch_labels = np.array(batch_labels, dtype=np.float32).reshape(-1)

                # Yield the data
                yield ((batch_input_target, batch_input_context), batch_labels)

                # Reset the batches for the next batch
                batch_input_target = []
                batch_input_context = []
                batch_labels = []

"""Defining the TensorFlow Dataset from the generator results, Allows us to feed the data efficiently to the model"""

def get_tf_dataset(batch_size):
    output_signature = (
        (tf.TensorSpec(shape=(None,), dtype=tf.int32),  # Target words
         tf.TensorSpec(shape=(None,), dtype=tf.int32)),  # Context words
        tf.TensorSpec(shape=(None,), dtype=tf.float32)  # Labels
    )
    return tf.data.Dataset.from_generator(
        lambda: data_generator(batch_size),  # The generator function
        output_signature=output_signature
    )

"""Final prep and training:"""

#Prefetching - allows us to prepae the next batch while the current is used for training
dataset = get_tf_dataset(batch_size=batch_size).prefetch(tf.data.experimental.AUTOTUNE)
#Build the Skipgram model (via build_skipgram_model we defined earlier)
skipgram_model = build_skipgram_model(vocabulary_size=len(word_to_index), embedding_dim=embedding_dim, learning_rate=learning_rate)

# Define callbacks for training
tensorboard_callback = TensorBoard(log_dir="model_logs") #logs metrics to tenzorboard (for possible checking/ visualization)
checkpoint_cb = ModelCheckpoint(filepath="word2vec_model_w5_ns15_ckpt.keras",
                                monitor='accuracy',
                                save_best_only=True) #save model checkpoints
early_stopping_cb = EarlyStopping(patience=3, restore_best_weights=True, monitor='accuracy') # stop early is the model doesn't improve 3 epochs in a row

#Train the model using the dataset and callbacks
history = skipgram_model.fit(dataset,
                             steps_per_epoch=steps_per_epoch,
                             epochs=epochs,
                             callbacks=[tensorboard_callback, checkpoint_cb, early_stopping_cb])

"""Saving embeddings:"""

# Extract the embeddings from the embedding layer
embedding_layer = skipgram_model.get_layer("word_embedding")
embeddings = embedding_layer.get_weights()[0]  # This gives you the embedding matrix (vocabulary_size x embedding_dim)

# Prepare a DataFrame to map words to their embeddings
index_to_word = {idx: word for word, idx in word_to_index.items()}
# Create a DataFrame where each row corresponds to a word and its embedding
embedding_df = pd.DataFrame(embeddings)
embedding_df['word'] = [index_to_word.get(i, '<UNK>') for i in range(len(embeddings))]
#Save the embeddings DataFrame as a Parquet file
parquet_file_path = "word_embeddings.parquet"
embedding_df.to_parquet(parquet_file_path, index=False)
print(f"Embeddings saved to {parquet_file_path}")

"""<h1 style="font-size:2rem;color:orange;">Find Most Similar</h1>

Defining the function:
"""

def find_most_similar_words_parquet(parquet_file, word_to_index, index_to_word):
    # List of special tokens
    special_tokens = {'<STOP>', '<UNK>', '<PAD>'}

    # Load the embeddings from the Parquet file
    embeddings_df = pd.read_parquet(parquet_file)

    # Separate the 'word' column and the embedding values
    words = embeddings_df['word'].values
    embeddings = embeddings_df.drop(columns=['word']).values  # Drop the 'word' column to get the embeddings matrix

    # Get inputs for the word and the number of similar words (k)
    word = input("Enter the word: ").strip().lower()  # Ensure lowercase input to match vocabulary

    try:
        k = int(input("Enter the number of similar words (k): ").strip())
    except ValueError:
        print("Invalid input for 'k'. Please enter an integer.")
        return

    # Check if the word exists in the vocabulary
    if word not in word_to_index:
        print(f"'{word}' is not in the vocabulary.")
        return

    # Get the index for the input word
    word_index = word_to_index[word]

    # Get the embedding for the input word
    target_embedding = embeddings[word_index].reshape(1, -1)

    # Compute cosine similarity between the input word's embedding and all other word embeddings
    similarities = cosine_similarity(target_embedding, embeddings)[0]

    # Get the indices of the top k most similar words, excluding special tokens and the input word itself
    most_similar_indices = [
        idx for idx in similarities.argsort()[::-1]
        if index_to_word[idx] not in special_tokens and idx != word_index
    ]

    # Keep only the top k results after filtering out special tokens and the input word itself
    most_similar_indices = most_similar_indices[:k]

    # Map the indices back to words and return with their similarity scores
    most_similar_words = [(index_to_word[idx], similarities[idx]) for idx in most_similar_indices]

    # Display the most similar words and their scores
    print(f"\nTop {k} words similar to '{word}':")
    for i, (similar_word, score) in enumerate(most_similar_words, 1):
        print(f"{i}. {similar_word} (cosine similarity: {score:.4f})")

    return most_similar_words

"""Calling the function:"""

parquet_file = "word_embeddings.parquet"
similar_words = find_most_similar_words_parquet(parquet_file, word_to_index, index_to_word)

"""<h1 style="font-size:1rem;color:orange;">T-nse - interesting clusters</h1>"""

df = pd.read_parquet(parquet_file)

words = df['word'].values
embeddings = df.drop(columns=['word']).values

# The words we want to visualize
words_to_visualize = ['food', 'breakfast', 'restaurant', 'bar', 'hotel', 'location', 'stay', 'area', 'downtown', 'lunch', 'room', 'suite']
word_indices = [np.where(words == word)[0][0] for word in words_to_visualize]

# Select the embeddings for these words
selected_embeddings = embeddings[word_indices]
print(f"Selected {len(words_to_visualize)} words for visualization.")

#Apply t-SNE to reduce the embedding dimensions to 2
tsne = TSNE(n_components=2, perplexity=3, random_state=42)
word_embeddings_2d = tsne.fit_transform(selected_embeddings)
print(f"Reduced embedding shape: {word_embeddings_2d.shape}")

# Step 4: Plot the 2D word embeddings with matplotlib
plt.figure(figsize=(10, 8))

# Scatter plot
plt.scatter(word_embeddings_2d[:, 0], word_embeddings_2d[:, 1], c='blue', edgecolors='k', s=100)

# Annotate each point with the corresponding word
for i, word in enumerate(words_to_visualize):
    plt.annotate(
        word,
        (word_embeddings_2d[i, 0], word_embeddings_2d[i, 1]),
        textcoords="offset points",
        xytext=(2, 7),
        ha='center'
    )

plt.title('t-SNE 2D Visualization of Specific Word Embeddings')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.show()

# Avoid memory leak warnings
import os
os.environ['OMP_NUM_THREADS'] = '2'  # Set the number of threads to 2

# Load the embeddings and words
parquet_file = "word_embeddings.parquet"
df = pd.read_parquet(parquet_file)
words = df['word'].values
embeddings = df.drop(columns=['word']).values

# Limit the subset of the vocabulary
top_n_words = 500
limited_word_indices = np.arange(top_n_words)
limited_word_embeddings = embeddings[limited_word_indices]

#Apply K-Means clustering
num_clusters = 10
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(limited_word_embeddings)

# Get cluster labels for each word
cluster_labels = kmeans.labels_

#Define the clusters to plot
clusters_to_plot = [0, 5, 6, 9]  # Specify clusters to plot
new_cluster_indices = {old: new + 1 for new, old in enumerate(clusters_to_plot)}  # Reset cluster index

# Limit to the top 5 words per cluster
words_per_cluster = {i: [] for i in clusters_to_plot}
embeddings_per_cluster = {i: [] for i in clusters_to_plot}

for i, word in enumerate(words[:top_n_words]):
    cluster_id = cluster_labels[i]
    if cluster_id in clusters_to_plot and len(words_per_cluster[cluster_id]) < 5:  # Limit to top 5 words
        words_per_cluster[cluster_id].append(word)
        embeddings_per_cluster[cluster_id].append(limited_word_embeddings[i])

# Print out only the selected clusters
for old_cluster_id in clusters_to_plot:
    new_cluster_id = new_cluster_indices[old_cluster_id]
    print(f"\nCluster {new_cluster_id}:")
    print(", ".join(words_per_cluster[old_cluster_id]))  # Print top 5 words in each selected cluster

# Apply t-SNE on the embeddings for visualization
all_selected_embeddings = np.vstack([np.array(embeddings_per_cluster[cluster]) for cluster in clusters_to_plot])
tsne = TSNE(n_components=3, perplexity=5, random_state=42)  # Adjust perplexity
word_embeddings_3d = tsne.fit_transform(all_selected_embeddings)

# Plot the t-SNE results
fig = plt.figure(figsize=(10, 12))
ax = fig.add_subplot(111, projection='3d')

# Plot only the selected clusters
start_idx = 0
for old_cluster_id in clusters_to_plot:
    new_cluster_id = new_cluster_indices[old_cluster_id]
    num_points = len(embeddings_per_cluster[old_cluster_id])
    cluster_points = word_embeddings_3d[start_idx:start_idx + num_points]
    start_idx += num_points
    if len(cluster_points) > 0:  # Ensure there are points in the cluster
        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], cluster_points[:, 2], label=f"Cluster {new_cluster_id}", s=50)

ax.set_title(f't-SNE 3D Visualization of Specific Clusters (Top 5 Words Each)')
ax.set_xlabel('t-SNE Dimension 1')
ax.set_ylabel('t-SNE Dimension 2')
ax.set_zlabel('t-SNE Dimension 3')

plt.legend()
plt.show()

"""<h1 style="font-size:1rem;color:orange;">Attachment - Randomsearch Hyperparameter Tuning</h1>

As mensioned abobe, we conducted a randomsearch to find the best hyperparameters:
"""

learning_rate = 0.0001  # Lower learning rate for more stable convergence
epochs = 30

def get_tf_dataset(window_size, number_of_neg_samples, batch_size):
    return tf.data.Dataset.from_generator(
        lambda: data_generator(batch_size, window_size, number_of_neg_samples),  # The generator function
        output_signature=(
            (tf.TensorSpec(shape=(None,), dtype=tf.int32),  # Target words
             tf.TensorSpec(shape=(None,), dtype=tf.int32)),  # Context words
            tf.TensorSpec(shape=(None,), dtype=tf.float32)  # Labels
        )
    ).prefetch(tf.data.experimental.AUTOTUNE)

#Initialize the hypermodel
hypermodel = SkipgramHyperModel(
    vocabulary_size=len(word_to_index),
    word_to_index=word_to_index
)

# Setup the tuner
tuner = RandomSearch(
    hypermodel,
    objective='accuracy',
    max_trials=10,  # Number of combinations to try
    executions_per_trial=1,  # How many models to train per trial
    directory='my_tuner_dir',  # Directory to store results
    project_name='word2vec_tuning'
)

# Define callbacks for training
tensorboard_callback = TensorBoard(log_dir="model_logs")
checkpoint_cb = ModelCheckpoint(filepath="word2vec_best_model.keras", monitor='accuracy', save_best_only=True)
early_stopping_cb = EarlyStopping(patience=3, restore_best_weights=True, monitor='accuracy')
callbacks = [tensorboard_callback, checkpoint_cb, early_stopping_cb]

# Run the tuner to search for the best hyperparameters
tuner.search(
    epochs=20,  # Maximum number of epochs
    callbacks=callbacks
)

# Retrieve and save the best model
best_model = tuner.get_best_models(num_models=1)[0]
best_model.save('best_word2vec_model.keras', include_optimizer=True)

#Print the best hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters:")
print(f"Embedding Dimension: {best_hyperparameters.get('embedding_dim')}")
print(f"Learning Rate: {best_hyperparameters.get('learning_rate')}")
print(f"Number of Negative Samples: {best_hyperparameters.get('number_of_neg_samples')}")
print(f"Window Size: {best_hyperparameters.get('window_size')}")
print(f"Batch Size: {best_hyperparameters.get('batch_size')}")